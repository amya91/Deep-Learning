{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.color import gray2rgb\n",
    "from skimage.color import rgb2hsv\n",
    "from skimage.transform import resize\n",
    "from skimage.viewer import ImageViewer\n",
    "import os\n",
    "import skimage\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading  n00015388  ...\n",
      "1000\n",
      "Reading  n00007846  ...\n",
      "2000\n",
      "Reading  n00017222  ...\n",
      "3000\n",
      "4000\n",
      "Reading  n00523513  ...\n",
      "5000\n",
      "Reading  n12992868  ...\n",
      "6000\n",
      "7000\n",
      "Reading  n09287968  ...\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "image_dim = 64\n",
    "imgs = np.zeros((8987,image_dim,image_dim,3))\n",
    "k=0\n",
    "y = np.zeros((8987,1))\n",
    "for l,i in enumerate(os.listdir(\"Images\")):\n",
    "    print(\"Reading \",i,\" ...\")\n",
    "    for j in os.listdir(os.path.join(\"Images\",i)):\n",
    "        x = imread(os.path.join('Images',i,j))\n",
    "        if len(x.shape) == 2:\n",
    "            x = gray2rgb(x)\n",
    "        imgs[k] = resize(x, (image_dim,image_dim,3)).reshape((1,image_dim,image_dim,3))\n",
    "        y[k] = l\n",
    "        k=k+1\n",
    "        if(k%1000 == 0):\n",
    "            print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test,y_train,y_test= train_test_split(imgs,y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#euclidean predictions\n",
    "def nn_pred_euclidean(reduced_img_tr,reduced_img_te):\n",
    "    pred = np.zeros((len(x_test),1)).astype('int')\n",
    "    for i,v in enumerate(reduced_img_te):\n",
    "        a = reduced_img_tr - v\n",
    "        b = np.linalg.norm(a,axis = 1).argsort()[0:5]\n",
    "        b = list(y_train[b].reshape(5,).astype('int'))\n",
    "        pred[i] = max(b,key = b.count)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pearson predictions\n",
    "def nn_pred_pear(reduced_img_tr,reduced_img_te):\n",
    "    Y_Y = reduced_img_tr - np.mean(reduced_img_tr,axis = 1).reshape(7189,1)\n",
    "    pred = np.zeros((len(reduced_img_te),1)).astype('int')\n",
    "    for i,v in enumerate(reduced_img_te):\n",
    "        X_X = (v - np.mean(v))\n",
    "        num = np.dot((v - np.mean(v)).reshape(1,reduced_img_te.shape[1]),reduced_img_tr.transpose())\n",
    "        Y_norm = np.linalg.norm(Y_Y,axis = 1)\n",
    "        X_norm = np.linalg.norm(X_X)\n",
    "        den = X_norm*Y_norm\n",
    "        b = (num/den).reshape(len(reduced_img_tr),).argsort()[-5:][::-1]\n",
    "        b = list(y_train[b].reshape(5,).astype('int'))\n",
    "        pred[i] = max(b,key = b.count)\n",
    "    return pred    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOENCODER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used the architecture which gave the least test mse for last question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Autoencoders \n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "input_img = Input(shape=(image_dim, image_dim, 3))\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "encoded = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, outputs = decoded)\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy', metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7189 samples, validate on 1798 samples\n",
      "Epoch 1/13\n",
      "7189/7189 [==============================] - 179s - loss: 0.6594 - mean_squared_error: 0.0630 - val_loss: 0.6106 - val_mean_squared_error: 0.0392\n",
      "Epoch 2/13\n",
      "7189/7189 [==============================] - 191s - loss: 0.6067 - mean_squared_error: 0.0391 - val_loss: 0.5911 - val_mean_squared_error: 0.0307\n",
      "Epoch 3/13\n",
      "7189/7189 [==============================] - 185s - loss: 0.5907 - mean_squared_error: 0.0322 - val_loss: 0.5933 - val_mean_squared_error: 0.0315\n",
      "Epoch 4/13\n",
      "7189/7189 [==============================] - 181s - loss: 0.5818 - mean_squared_error: 0.0285 - val_loss: 0.5806 - val_mean_squared_error: 0.0265\n",
      "Epoch 5/13\n",
      "7189/7189 [==============================] - 203s - loss: 0.5781 - mean_squared_error: 0.0271 - val_loss: 0.5773 - val_mean_squared_error: 0.0249\n",
      "Epoch 6/13\n",
      "7189/7189 [==============================] - 177s - loss: 0.5731 - mean_squared_error: 0.0251 - val_loss: 0.5754 - val_mean_squared_error: 0.0245\n",
      "Epoch 7/13\n",
      "7189/7189 [==============================] - 199s - loss: 0.5705 - mean_squared_error: 0.0240 - val_loss: 0.5716 - val_mean_squared_error: 0.0228\n",
      "Epoch 8/13\n",
      "7189/7189 [==============================] - 185s - loss: 0.5679 - mean_squared_error: 0.0230 - val_loss: 0.5741 - val_mean_squared_error: 0.0238\n",
      "Epoch 9/13\n",
      "7189/7189 [==============================] - 178s - loss: 0.5662 - mean_squared_error: 0.0223 - val_loss: 0.5727 - val_mean_squared_error: 0.0232\n",
      "Epoch 10/13\n",
      "7189/7189 [==============================] - 175s - loss: 0.5644 - mean_squared_error: 0.0216 - val_loss: 0.5687 - val_mean_squared_error: 0.0215\n",
      "Epoch 11/13\n",
      "7189/7189 [==============================] - 169s - loss: 0.5634 - mean_squared_error: 0.0212 - val_loss: 0.5636 - val_mean_squared_error: 0.0196\n",
      "Epoch 12/13\n",
      "7189/7189 [==============================] - 175s - loss: 0.5618 - mean_squared_error: 0.0206 - val_loss: 0.5634 - val_mean_squared_error: 0.0196\n",
      "Epoch 13/13\n",
      "7189/7189 [==============================] - 181s - loss: 0.5606 - mean_squared_error: 0.0202 - val_loss: 0.5659 - val_mean_squared_error: 0.0207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f105beb5400>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=13,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded_imgs_test = encoder.predict(x_test).reshape((len(x_test),np.prod([16,16,32])))\n",
    "encoded_imgs_train = encoder.predict(x_train).reshape((len(x_train),np.prod([16,16,32])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix 5-nn predictions using euclidean distance for autoencoders \n",
      " [[116  17  47  17  17 104]\n",
      " [ 41  53  22  19  14  82]\n",
      " [ 50   8  99  11  27  63]\n",
      " [ 84  13  40  77  16 158]\n",
      " [ 46   7  54   8  64  68]\n",
      " [ 29   9  26   8  10 274]]\n",
      "\n",
      "Accuracy using euclidean distance for autoencoders =  37.9866518354 %\n"
     ]
    }
   ],
   "source": [
    "pred_AE_eu = nn_pred_euclidean(encoded_imgs_train,encoded_imgs_test)\n",
    "cf_mat_AE_eu = confusion_matrix(y_test, pred_AE_eu)\n",
    "\n",
    "print (\"Confusion matrix 5-nn predictions using euclidean distance for autoencoders \\n\",cf_mat_AE_eu)\n",
    "print(\"\\nAccuracy using euclidean distance for autoencoders = \",\n",
    "      sum(np.diagonal(cf_mat_AE_eu))/len(x_test)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix 5-nn predictions using pearson correlation for autoencoders [[ 82  39  36  29  21 111]\n",
      " [ 26  87  11  20  16  71]\n",
      " [ 31  15 102  11  35  64]\n",
      " [ 59  24  33 122  19 131]\n",
      " [ 35  17  49   6  83  57]\n",
      " [ 30  17  17  10   9 273]]\n",
      "\n",
      "Accuracy using pearson correlation for autoencoders =  41.6573971079 %\n"
     ]
    }
   ],
   "source": [
    "pred_AE_pe = nn_pred_pear(encoded_imgs_train,encoded_imgs_test)\n",
    "cf_mat_AE_pe = confusion_matrix(y_test, pred_AE_pe)\n",
    "\n",
    "print (\"Confusion matrix 5-nn predictions using pearson correlation for autoencoders\", cf_mat_AE_pe)\n",
    "print(\"\\nAccuracy using pearson correlation for autoencoders = \",\n",
    "      sum(np.diagonal(cf_mat_AE_pe))/len(x_test)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SVD\n",
    "import tensorflow as tf\n",
    "x_train_svd = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test_svd = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=60, n_iter=7,\n",
       "       random_state=42, tol=0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "svd = TruncatedSVD(n_components=60, n_iter=7, random_state=42)\n",
    "svd.fit(x_train_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why 60 Components?**\n",
    "\n",
    "I tried with different values of eigenvectors i.e. n_components and got best accuracy with 60. Hence I have used n_components = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Explained by 60 components 0.688740006381\n"
     ]
    }
   ],
   "source": [
    "#Variance explaiend by 60 components\n",
    "print(\"Variance Explained by 60 components\",svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_transformed = svd.transform(x_train_svd)\n",
    "test_transformed = svd.transform(x_test_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix 5-nn predictions using euclidean distance for SVD Vectors \n",
      " [[ 77  15  73  43  34  76]\n",
      " [ 33  43  28  43  30  54]\n",
      " [ 36   5 124  18  35  40]\n",
      " [ 72   4  47 151  24  90]\n",
      " [ 26   5  57  24  95  40]\n",
      " [ 24  11  22  36  13 250]]\n",
      "\n",
      "Accuracy using euclidean distance for RGB Vectors =  41.1568409344 %\n"
     ]
    }
   ],
   "source": [
    "pred_SVD_eu = nn_pred_euclidean(train_transformed,test_transformed)\n",
    "cf_mat_SVD_eu = confusion_matrix(y_test, pred_SVD_eu)\n",
    "\n",
    "print (\"Confusion matrix 5-nn predictions using euclidean distance for SVD Vectors \\n\",cf_mat_SVD_eu)\n",
    "print(\"\\nAccuracy using euclidean distance for RGB Vectors = \",\n",
    "      sum(np.diagonal(cf_mat_SVD_eu))/len(x_test)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix 5-nn predictions using euclidean distance for SVD Vectors \n",
      " [[ 90  15  51  46  25  91]\n",
      " [ 54  34  17  34  23  69]\n",
      " [ 29   3 126  17  36  47]\n",
      " [ 54  10  33 165  11 115]\n",
      " [ 35   6  42  25  80  59]\n",
      " [ 28  13  14  29   6 266]]\n",
      "\n",
      "Accuracy using euclidean distance for RGB Vectors =  42.3248053393 %\n"
     ]
    }
   ],
   "source": [
    "pred_SVD_pe = nn_pred_pear(train_transformed,test_transformed)\n",
    "cf_mat_SVD_pe = confusion_matrix(y_test, pred_SVD_pe)\n",
    "\n",
    "print (\"Confusion matrix 5-nn predictions using euclidean distance for SVD Vectors \\n\",cf_mat_SVD_pe)\n",
    "print(\"\\nAccuracy using euclidean distance for RGB Vectors = \",\n",
    "      sum(np.diagonal(cf_mat_SVD_pe))/len(x_test)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#converting xtrain into rgb\n",
    "x_train_rgb = np.zeros((len(x_train),256*3))\n",
    "for idx,i in enumerate(x_train):\n",
    "    i = skimage.img_as_ubyte(i) #as the values were normalized after resizing converting them back into rgb\n",
    "    # creating a dictionary to count the frequency of each value from 0-255\n",
    "    r = Counter(i[:,:,0].reshape(i.shape[0]*i.shape[1])) \n",
    "    g = Counter(i[:,:,1].reshape(i.shape[0]*i.shape[1]))\n",
    "    b = Counter(i[:,:,2].reshape(i.shape[0]*i.shape[1]))\n",
    "    rgb = np.zeros((3,256))\n",
    "    #creating rgb vectors seperately\n",
    "    rgb[0,np.array(list(r.keys()))] = list(r.values())\n",
    "    rgb[1,np.array(list(g.keys()))] = list(g.values())\n",
    "    rgb[2,np.array(list(b.keys()))] = list(b.values())\n",
    "    rgb = rgb.reshape(256*3)\n",
    "    x_train_rgb[idx] = rgb\n",
    "    \n",
    "x_test_rgb = np.zeros((len(x_test),256*3))\n",
    "for idx,i in enumerate(x_test):\n",
    "    i = skimage.img_as_ubyte(i) #as the values were normalized after resizing converting them back into rgb\n",
    "    # creating a dictionary to count the frequency of each value from 0-255\n",
    "    r = Counter(i[:,:,0].reshape(64*64)) \n",
    "    g = Counter(i[:,:,1].reshape(64*64))\n",
    "    b = Counter(i[:,:,2].reshape(64*64))\n",
    "    rgb = np.zeros((3,256))\n",
    "    #creating rgb vectors seperately\n",
    "    rgb[0,np.array(list(r.keys()))] = list(r.values())\n",
    "    rgb[1,np.array(list(g.keys()))] = list(g.values())\n",
    "    rgb[2,np.array(list(b.keys()))] = list(b.values())\n",
    "    rgb = rgb.reshape(256*3)\n",
    "    x_test_rgb[idx] = rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix 5-nn predictions using euclidean distance for RGB Vectors \n",
      " [[ 84  21  44  26  87  56]\n",
      " [ 26  58  13  26  77  31]\n",
      " [ 25   9 118  10  66  30]\n",
      " [ 73  33  37 113  88  44]\n",
      " [ 28  13  29   7 150  20]\n",
      " [ 61  24  34  28  59 150]]\n",
      "\n",
      "Accuracy using euclidean distance for RGB Vectors =  37.4304783092 %\n"
     ]
    }
   ],
   "source": [
    "pred_RGB_eu = nn_pred_euclidean(x_train_rgb,x_test_rgb)\n",
    "cf_mat_RGB_eu = confusion_matrix(y_test, pred_RGB_eu)\n",
    "\n",
    "print (\"Confusion matrix 5-nn predictions using euclidean distance for RGB Vectors \\n\",cf_mat_RGB_eu)\n",
    "print(\"\\nAccuracy using euclidean distance for RGB Vectors = \",\n",
    "      sum(np.diagonal(cf_mat_RGB_eu))/len(x_test)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix 5-nn predictions using euclidean distance for RGB Vectors\n",
      " [[ 85  29  49  28  53  74]\n",
      " [ 32  76  11  19  49  44]\n",
      " [ 35  10 112  13  51  37]\n",
      " [ 69  32  50 126  47  64]\n",
      " [ 37  17  36   4 117  36]\n",
      " [ 55  28  33  24  28 188]]\n",
      "\n",
      "Accuracy using euclidean distance for RGB Vectors =  39.1546162403 %\n"
     ]
    }
   ],
   "source": [
    "pred_RGB_pe = nn_pred_pear(x_train_rgb,x_test_rgb)\n",
    "cf_mat_RGB_pe = confusion_matrix(y_test, pred_RGB_pe)\n",
    "\n",
    "print (\"Confusion matrix 5-nn predictions using euclidean distance for RGB Vectors\\n\",cf_mat_RGB_pe)\n",
    "print(\"\\nAccuracy using euclidean distance for RGB Vectors = \",\n",
    "      sum(np.diagonal(cf_mat_RGB_pe))/len(x_test)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#converting xtrain into rgb\n",
    "x_train_hsv = np.zeros((len(x_train),(256*2+180)))\n",
    "for idx,i in enumerate(x_train):\n",
    "    h, _ = np.histogram(i[:,:,0], bins = 180 )\n",
    "    s, _ = np.histogram(i[:,:,1], bins = 256 )\n",
    "    v, _ = np.histogram(i[:,:,2], bins = 256 )\n",
    "    x_train_hsv[idx] = np.concatenate((h,s,v), axis=0)\n",
    "    \n",
    "x_test_hsv = np.zeros((len(x_test),(256*2+180)))\n",
    "for idx,i in enumerate(x_test):\n",
    "    h, _ = np.histogram(i[:,:,0], bins = 180 )\n",
    "    s, _ = np.histogram(i[:,:,1], bins = 256 )\n",
    "    v, _ = np.histogram(i[:,:,2], bins = 256 )\n",
    "    x_test_hsv[idx] = np.concatenate((h,s,v), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for HSV Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix 5-nn predictions using euclidean distance for HSV Vectors \n",
      " [[ 82  26  47  18  79  66]\n",
      " [ 40  57  14  22  67  31]\n",
      " [ 37   7 102  11  61  40]\n",
      " [ 77  29  25 115  93  49]\n",
      " [ 35  13  35   6 138  20]\n",
      " [ 60  26  25  23  51 171]]\n",
      "\n",
      "Accuracy using euclidean distance for RGB Vectors =  36.9855394883 %\n"
     ]
    }
   ],
   "source": [
    "pred_HSV_eu = nn_pred_euclidean(x_train_hsv,x_test_hsv)\n",
    "cf_mat_HSV_eu = confusion_matrix(y_test, pred_HSV_eu)\n",
    "\n",
    "print (\"Confusion matrix 5-nn predictions using euclidean distance for HSV Vectors \\n\",cf_mat_HSV_eu)\n",
    "print(\"\\nAccuracy using euclidean distance for RGB Vectors = \",\n",
    "      sum(np.diagonal(cf_mat_HSV_eu))/len(x_test)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix 5-nn predictions using euclidean distance for HSV Vectors \n",
      " [[ 76  21  56  32  57  76]\n",
      " [ 39  72  14  17  49  40]\n",
      " [ 35  13 105  21  48  36]\n",
      " [ 80  32  34 139  50  53]\n",
      " [ 26  21  37   6 122  35]\n",
      " [ 54  24  27  19  31 201]]\n",
      "\n",
      "Accuracy using euclidean distance for RGB Vectors =  39.766407119 %\n"
     ]
    }
   ],
   "source": [
    "pred_HSV_pe = nn_pred_pear(x_train_hsv,x_test_hsv)\n",
    "cf_mat_HSV_pe = confusion_matrix(y_test, pred_HSV_pe)\n",
    "\n",
    "print (\"Confusion matrix 5-nn predictions using euclidean distance for HSV Vectors \\n\",cf_mat_HSV_pe)\n",
    "print(\"\\nAccuracy using euclidean distance for RGB Vectors = \",\n",
    "      sum(np.diagonal(cf_mat_HSV_pe))/len(x_test)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Accuracy for different methods\n",
    "\n",
    "                                 \n",
    "                  \n",
    "1] Autoencoder  :  Euclidean  37.98% , Pearson      41.66%\n",
    "\n",
    "2] SVD          :  Euclidean  41.16% , Pearson      42.32%\n",
    "\n",
    "3] RGB Histogram:  Euclidean  37.43% , Pearson      39.15%\n",
    "\n",
    "4] HSV Histogram:  Euclidean  36.96% , Pearson 39.77%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest accuracy for test data was obtained for SVD vectors. Train and test data used for each of the method is same"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
